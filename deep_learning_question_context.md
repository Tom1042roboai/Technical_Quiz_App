# Deep Learning - Context and Overview

## Core Concepts in Deep Learning

### 1. Neural Network Fundamentals
- **Architecture**: Feedforward, recurrent, convolutional networks
- **Activation Functions**: ReLU, sigmoid, tanh, softmax
- **Backpropagation**: The learning algorithm for neural networks
- **Optimization**: SGD, Adam, RMSprop, and other optimizers

### 2. Advanced Architectures
- **CNNs**: Convolutional Neural Networks for computer vision
- **RNNs/LSTMs/GRUs**: Recurrent networks for sequential data
- **Transformers**: Self-attention based architectures
- **Autoencoders & GANs**: For unsupervised learning and generation

### 3. Training Deep Networks
- **Initialization**: Xavier/Glorot, He initialization
- **Regularization**: Dropout, weight decay, batch normalization
- **Loss Functions**: Cross-entropy, MSE, contrastive loss
- **Learning Rate Scheduling**: Step, cosine, and cyclic schedules

## Applications
- Computer Vision: Image classification, object detection
- Natural Language Processing: Translation, text generation
- Speech Recognition: Voice assistants, transcription
- Reinforcement Learning: Game playing, robotics

## Question Difficulty Distribution

| Difficulty | Count | Percentage | Time Limit (seconds) |
|------------|-------|------------|---------------------|
| Easy       | 10    | 31.25%     | 15                  |
| Medium     | 11    | 34.38%     | 20                  |
| Hard       | 11    | 34.38%     | 25                  |

## Learning Resources
- **Books**: "Deep Learning" by Goodfellow et al., "Neural Networks and Deep Learning" by Nielsen
- **Courses**: Deep Learning Specialization (deeplearning.ai), Fast.ai
- **Frameworks**: PyTorch, TensorFlow, JAX
